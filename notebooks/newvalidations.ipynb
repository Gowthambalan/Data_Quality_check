{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1a1ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea8235ac",
   "metadata": {},
   "source": [
    "Validations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2c60e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed975f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ed6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e76c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class FileValidator:\n",
    "\n",
    "    def __init__(self, file_path, known_headers=[\"MeterNumber\", \"Meter ID\", \"Meter_ID\", \"Meter_Number\"]):\n",
    "        self.file_path = file_path\n",
    "        self.known_headers = known_headers if known_headers else [\"nothing\"]\n",
    "        self.errors = []\n",
    "        self.status = \"PASS\" \n",
    "\n",
    "    # 1. Validate File Extension\n",
    "    def validate_file_type(self):\n",
    "        valid_extensions = ['.xlsx', '.xls', '.csv']\n",
    "        _, file_ext = os.path.splitext(self.file_path)\n",
    "\n",
    "        if file_ext.lower() not in valid_extensions:\n",
    "            self.errors.append(f\" Unsupported file type: {file_ext}\")\n",
    "            self.status = \"REJECT\"\n",
    "            return False\n",
    "        \n",
    "        print(f\" File type valid: {file_ext}\")\n",
    "        return True\n",
    "\n",
    "    # 2. Validate Header Presence\n",
    "    def validate_header_presence(self):\n",
    "        try:\n",
    "            if self.file_path.endswith('.csv'):\n",
    "                df = pd.read_csv(self.file_path)\n",
    "            else:\n",
    "                df = pd.read_excel(self.file_path)\n",
    "\n",
    "            headers = df.columns.str.lower()\n",
    "            if any(header.lower() in headers for header in self.known_headers):\n",
    "                print(\" Header contains at least one known meter field.\")\n",
    "                return True\n",
    "            else:\n",
    "                self.errors.append(\"No known meter headers found.\")\n",
    "                self.status = \"ROUTE_TO_HEADER_MAPPING_AI\"\n",
    "                return False\n",
    "\n",
    "        except Exception as e:\n",
    "            self.errors.append(f\" Error reading file: {str(e)}\")\n",
    "            self.status = \"REJECT\"\n",
    "            return False\n",
    "\n",
    "    # 3. Run All Validations\n",
    "    def run_all_validations(self):\n",
    "        print(\"\\n--- Running File Validations ---\")\n",
    "        if not self.validate_file_type():\n",
    "            return self.status, self.errors\n",
    "        \n",
    "        self.validate_header_presence()\n",
    "        return self.status, self.errors\n",
    "\n",
    "\n",
    "#  Example Usage:\n",
    "file_path = r\"D:\\AI Projects\\powerthon\\VALIDATIONS\\Powerthon_IP_DATA_Adani_Project_V2.xlsx\"  # Change to your test file path\n",
    "validator = FileValidator(file_path)\n",
    "\n",
    "status, errors = validator.run_all_validations()\n",
    "\n",
    "print(\"\\n--- Validation Result ---\")\n",
    "print(\"Final Status:\", status)\n",
    "print(\"Errors / Notes:\", errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0369e063",
   "metadata": {},
   "source": [
    "header_Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770885cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class HeaderNormalizer:\n",
    "    def __init__(self, file_path, header_mapping):\n",
    "        \"\"\"\n",
    "        :param file_path: Path of the input file\n",
    "        :param header_mapping: Dictionary of vendor header ‚Üí canonical header\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.header_mapping = {key.lower(): value for key, value in header_mapping.items()}\n",
    "        self.original_headers = []\n",
    "        self.mapped_headers = []\n",
    "        self.status = \"NOT_STARTED\"\n",
    "\n",
    "    def load_file(self):\n",
    "        try:\n",
    "            if self.file_path.endswith('.csv'):\n",
    "                df = pd.read_csv(self.file_path)\n",
    "            else:\n",
    "                df = pd.read_excel(self.file_path)\n",
    "\n",
    "            self.original_headers = list(df.columns)\n",
    "            return True, df\n",
    "        except Exception as e:\n",
    "            self.status = \"FAILED\"\n",
    "            return False, str(e)\n",
    "\n",
    "    def normalize_headers(self, df):\n",
    "        canonical_headers = []\n",
    "        for col in df.columns:\n",
    "            normalized = self.header_mapping.get(col.lower(), col)  # Map or keep original\n",
    "            canonical_headers.append(normalized)\n",
    "\n",
    "        self.mapped_headers = canonical_headers\n",
    "        df.columns = canonical_headers  # Replace in dataframe\n",
    "        self.status = \"HEADERS_NORMALIZED\"\n",
    "        return df\n",
    "\n",
    "    def run(self):\n",
    "        print(\"\\n--- Header Normalization Process ---\")\n",
    "        \n",
    "        success, result = self.load_file()\n",
    "        if not success:\n",
    "            print(\" Error loading file:\", result)\n",
    "            return self.status\n",
    "        \n",
    "        print(\" File loaded successfully.\")\n",
    "        df = result\n",
    "        \n",
    "        df = self.normalize_headers(df)\n",
    "        print(\" Header mapping complete.\")\n",
    "        print(\"\\nOriginal Headers:\", self.original_headers)\n",
    "        print(\"Mapped Headers:   \", self.mapped_headers)\n",
    "\n",
    "        return df, self.original_headers, self.mapped_headers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6322579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b40a2665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileValidator:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.errors = []\n",
    "        self.status = \"PASS\"\n",
    "\n",
    "    def validate_file_type(self):\n",
    "        valid_extensions = ['.xlsx', '.xls', '.csv']\n",
    "        _, file_ext = os.path.splitext(self.file_path)\n",
    "\n",
    "        if file_ext.lower() not in valid_extensions:\n",
    "            self.errors.append(f\" Unsupported file type: {file_ext}\")\n",
    "            self.status = \"REJECT\"\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def validate_header_presence(self, known_headers=[\"MeterNumber\", \"Meter ID\", \"Meter_ID\", \"Meter_Number\"]):\n",
    "        known_headers = known_headers if known_headers else [\"meter_id\", \"meter_no\", \"meter_number\"]\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(self.file_path) if self.file_path.endswith('.csv') else pd.read_excel(self.file_path)\n",
    "            headers = df.columns.str.lower()\n",
    "\n",
    "            if any(h.lower() in headers for h in known_headers):\n",
    "                return True\n",
    "            else:\n",
    "                self.errors.append(\" No known meter headers found.\")\n",
    "                self.status = \"ROUTE_TO_HEADER_MAPPING_AI\"\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            self.errors.append(f\" Error reading file: {e}\")\n",
    "            self.status = \"REJECT\"\n",
    "            return False\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        print(\"üîç Running File-Level Validation...\")\n",
    "        if not self.validate_file_type():\n",
    "            return None, self.status, self.errors\n",
    "\n",
    "        file_readable = self.validate_header_presence()\n",
    "        df = None\n",
    "        if file_readable:\n",
    "            df = pd.read_csv(self.file_path) if self.file_path.endswith('.csv') else pd.read_excel(self.file_path)\n",
    "\n",
    "        return df, self.status, self.errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "75f20aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeaderNormalizer:\n",
    "    def __init__(self, df, header_mapping):\n",
    "        self.df = df\n",
    "        self.header_mapping = {k.lower(): v for k, v in header_mapping.items()}\n",
    "        self.original_headers = list(df.columns)\n",
    "        self.mapped_headers = []\n",
    "        self.status = \"PASS\"\n",
    "\n",
    "    def normalize_headers(self):\n",
    "        new_cols = []\n",
    "        for col in self.df.columns:\n",
    "            new_cols.append(self.header_mapping.get(col.lower(), col))\n",
    "        self.df.columns = new_cols\n",
    "        self.mapped_headers = new_cols\n",
    "        print(\" Header Normalization Completed\")\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e97347f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchemaValidator:\n",
    "    def __init__(self, df, mandatory_columns=None, duplicate_mapping=None):\n",
    "        self.df = df\n",
    "        self.logs = []\n",
    "        self.status = \"PASS\"\n",
    "        self.mandatory_columns = mandatory_columns or [\"MeterNumber\", \"Datetime\", \"Voltage\" ]\n",
    "        self.duplicate_mapping = duplicate_mapping or {\n",
    "            \"meter_no\": \"meter_id\",\n",
    "            \"meter_number\": \"meter_id\",\n",
    "            \"kwh\": \"energy\"\n",
    "        }\n",
    "\n",
    "    def check_mandatory(self):\n",
    "        missing = [col for col in self.mandatory_columns if col not in self.df.columns]\n",
    "        if missing:\n",
    "            self.logs.append(f\" Missing mandatory columns: {missing}\")\n",
    "            self.status = \"PARTIAL_INGEST\"\n",
    "        return missing\n",
    "\n",
    "    def remove_duplicate_columns(self):\n",
    "        for dup, main in self.duplicate_mapping.items():\n",
    "            if dup in self.df.columns and main in self.df.columns:\n",
    "                self.logs.append(f\" Duplicate: {dup} & {main}, keeping {main}\")\n",
    "                self.df.drop(columns=[dup], inplace=True)\n",
    "\n",
    "    def run(self):\n",
    "        print(\"üîç Running Schema-Level Validation...\")\n",
    "        self.check_mandatory()\n",
    "        self.remove_duplicate_columns()\n",
    "        return self.df, self.status, self.logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "88b2637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class RowValidator:\n",
    "    def __init__(self, df, meter_col=\"MeterNumber\", consumer_col=\"ConsumerID\", timestamp_col=\"Datetime\"):\n",
    "        self.df = df\n",
    "        self.logs = []\n",
    "        self.status = \"PASS\"\n",
    "        self.meter_col = meter_col\n",
    "        self.consumer_col = consumer_col\n",
    "        self.timestamp_col = timestamp_col\n",
    "\n",
    "    # 1. Timestamp Parse Validation\n",
    "    def parse_timestamp(self):\n",
    "        possible_formats = [\n",
    "            \"%Y-%m-%d %H:%M:%S\",\n",
    "            \"%d-%m-%Y %H:%M\",\n",
    "            \"%Y/%m/%d %H:%M\",\n",
    "            \"%d/%m/%Y %H:%M:%S\",\n",
    "            \"%m-%d-%Y %H:%M:%S\"\n",
    "        ]\n",
    "        parsed_timestamps = []\n",
    "        failed_rows = 0\n",
    "\n",
    "        for ts in self.df[self.timestamp_col]:\n",
    "            parsed = None\n",
    "            for fmt in possible_formats:\n",
    "                try:\n",
    "                    parsed = datetime.strptime(str(ts), fmt)\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            if parsed:\n",
    "                parsed_timestamps.append(parsed)\n",
    "            else:\n",
    "                parsed_timestamps.append(None)\n",
    "                failed_rows += 1\n",
    "\n",
    "        self.df[self.timestamp_col] = parsed_timestamps\n",
    "\n",
    "        if failed_rows > 0:\n",
    "            self.status = \"TIMESTAMP_FIXER\"\n",
    "            self.logs.append(f\" {failed_rows} timestamps could not be parsed. Send to 'timestamp fixer'.\")\n",
    "\n",
    "    # 2. Timestamp Continuity Check (30 min ¬± 5 min)\n",
    "    def check_timestamp_continuity(self):\n",
    "        self.df = self.df.sort_values(by=[self.meter_col, self.timestamp_col])\n",
    "        missing_intervals = 0\n",
    "\n",
    "        for meter_id, group in self.df.groupby(self.meter_col):\n",
    "            times = list(group[self.timestamp_col])\n",
    "            for i in range(1, len(times)):\n",
    "                if times[i] and times[i-1]:\n",
    "                    diff = (times[i] - times[i-1]).total_seconds() / 60\n",
    "                    if not (25 <= diff <= 35):  # 30 min ¬± 5 min\n",
    "                        missing_intervals += 1\n",
    "\n",
    "        if missing_intervals > 0:\n",
    "            self.status = \"MISSING_INTERVAL\"\n",
    "            self.logs.append(f\" {missing_intervals} timestamp gaps not within 30¬±5 minutes.\")\n",
    "\n",
    "    # 3. Meter ‚Üí Consumer Mapping Consistency\n",
    "    def check_meter_consumer_binding(self):\n",
    "        conflicts = 0\n",
    "        meter_map = {}\n",
    "\n",
    "        for _, row in self.df.iterrows():\n",
    "            meter = row[self.meter_col]\n",
    "            consumer = row[self.consumer_col]\n",
    "            if meter in meter_map:\n",
    "                if meter_map[meter] != consumer:\n",
    "                    conflicts += 1\n",
    "            else:\n",
    "                meter_map[meter] = consumer\n",
    "\n",
    "        if conflicts > 0:\n",
    "            self.status = \"ID_CONFLICT\"\n",
    "            self.logs.append(f\" {conflicts} meter-consumer conflicts found (same meter, different consumer)!\")\n",
    "\n",
    "    # Run all validations\n",
    "    def run(self):\n",
    "        print(\" Running Row-Level Validation...\")\n",
    "        self.parse_timestamp()\n",
    "        self.check_timestamp_continuity()\n",
    "        self.check_meter_consumer_binding()\n",
    "\n",
    "        return self.df, self.status, self.logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1ad3c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipeline:\n",
    "    def __init__(self, file_path, header_mapping):\n",
    "        self.file_path = file_path\n",
    "        self.header_mapping = header_mapping\n",
    "\n",
    "    def run(self):\n",
    "        print(\"\\n Starting Full Pipeline...\\n\")\n",
    "\n",
    "        # Step 1: File Validation\n",
    "        file_validator = FileValidator(self.file_path)\n",
    "        df, status, errors = file_validator.run()\n",
    "        if df is None:\n",
    "            return status, errors\n",
    "\n",
    "        # Step 2: Header Normalization\n",
    "        normalizer = HeaderNormalizer(df, self.header_mapping)\n",
    "        df = normalizer.normalize_headers()\n",
    "\n",
    "        # Step 3: Schema Validation\n",
    "        schema = SchemaValidator(df)\n",
    "        df, schema_status, schema_logs = schema.run()\n",
    "\n",
    "        # Step 4: Row-Level Validation\n",
    "        row_validator = RowValidator(df)\n",
    "        df, row_status, row_logs = row_validator.run()\n",
    "\n",
    "        final_status = \"PASS\" if status == \"PASS\" and schema_status == \"PASS\" and row_status == \"PASS\" else \"PARTIAL/FAIL\"\n",
    "        all_logs = errors + schema_logs + row_logs\n",
    "        return final_status, all_logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a5e9076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class ValueValidator:\n",
    "    def __init__(self, df, energy_col=\"Energy\", voltage_col=\"Voltage\",\n",
    "                 current_col=\"Current\", pf_col=\"PowerFactor\", freq_col=\"Frequency\"):\n",
    "        self.df = df\n",
    "        self.logs = []\n",
    "        self.status = \"PASS\"\n",
    "        self.energy_col = energy_col\n",
    "        self.voltage_col = voltage_col\n",
    "        self.current_col = current_col\n",
    "        self.pf_col = pf_col\n",
    "        self.freq_col = freq_col\n",
    "\n",
    "    # 1. Non-negative & Non-decreasing Energy\n",
    "    def check_energy_values(self):\n",
    "        if self.energy_col not in self.df.columns:\n",
    "            self.logs.append(f\"‚ùå Missing column: {self.energy_col}\")\n",
    "            self.status = \"FAIL\"\n",
    "            return\n",
    "\n",
    "        negative_values = (self.df[self.energy_col] < 0).sum()\n",
    "        if negative_values > 0:\n",
    "            self.logs.append(f\"‚ö† {negative_values} negative energy values found! (BAD_READING)\")\n",
    "            self.status = \"BAD_READING\"\n",
    "\n",
    "        decreasing = (self.df[self.energy_col].diff() < 0).sum()\n",
    "        if decreasing > 0:\n",
    "            self.logs.append(f\"‚ö† {decreasing} decreasing energy values detected! (ROLL_OVER)\")\n",
    "            self.status = \"ROLL_OVER\"\n",
    "\n",
    "    # 2. Interval Energy Limits (default: ‚â§10kWh per 30 min)\n",
    "    def check_interval_bounds(self, limit=10):\n",
    "        diff_values = self.df[self.energy_col].diff()\n",
    "        outliers = (diff_values > limit).sum()\n",
    "\n",
    "        if outliers > 0:\n",
    "            self.logs.append(f\"‚ö† {outliers} rows exceed {limit} kWh interval limit! (OUTLIER_INTERVAL)\")\n",
    "            self.status = \"OUTLIER_INTERVAL\"\n",
    "\n",
    "    # 3. Voltage Range (180 - 260V)\n",
    "    def check_voltage(self):\n",
    "        anomalies = ((self.df[self.voltage_col] < 180) | (self.df[self.voltage_col] > 260)).sum()\n",
    "        if anomalies > 0:\n",
    "            self.logs.append(f\"‚ö† {anomalies} voltage anomalies found! (VOLTAGE_ANOMALY)\")\n",
    "            self.status = \"VOLTAGE_ANOMALY\"\n",
    "\n",
    "    # 4. Current Range (0 - 60A)\n",
    "    def check_current(self, max_current=60):\n",
    "        anomalies = ((self.df[self.current_col] < 0) | \n",
    "                     (self.df[self.current_col] > max_current)).sum()\n",
    "        if anomalies > 0:\n",
    "            self.logs.append(f\"‚ö† {anomalies} current anomalies! (CURRENT_ANOMALY)\")\n",
    "            self.status = \"CURRENT_ANOMALY\"\n",
    "\n",
    "    # 5. Power Factor Range (-1 to 1)\n",
    "    def check_power_factor(self):\n",
    "        anomalies = ((self.df[self.pf_col] < -1) | (self.df[self.pf_col] > 1)).sum()\n",
    "        if anomalies > 0:\n",
    "            self.logs.append(f\"‚ö† {anomalies} PF values out of range! (PF_OUT_OF_RANGE)\")\n",
    "            self.status = \"PF_OUT_OF_RANGE\"\n",
    "\n",
    "    # 6. Frequency Range (49 - 51Hz)\n",
    "    def check_frequency(self):\n",
    "        anomalies = ((self.df[self.freq_col] < 49) | (self.df[self.freq_col] > 51)).sum()\n",
    "        if anomalies > 0:\n",
    "            self.logs.append(f\"‚ö† {anomalies} frequency anomalies! (FREQ_ANOMALY)\")\n",
    "            self.status = \"FREQ_ANOMALY\"\n",
    "\n",
    "    # Run all value validations\n",
    "    def run(self):\n",
    "        print(\"üîç Running Value-Level Validation...\")\n",
    "        self.check_energy_values()\n",
    "        self.check_interval_bounds()\n",
    "        self.check_voltage()\n",
    "        self.check_current()\n",
    "        self.check_power_factor()\n",
    "        self.check_frequency()\n",
    "\n",
    "        return self.df, self.status, self.logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5496e7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting Full Pipeline...\n",
      "\n",
      "üîç Running File-Level Validation...\n",
      " Header Normalization Completed\n",
      "üîç Running Schema-Level Validation...\n",
      " Running Row-Level Validation...\n",
      "\n",
      " Final Pipeline Status: PARTIAL/FAIL\n",
      " Logs: [\" 640292 timestamps could not be parsed. Send to 'timestamp fixer'.\"]\n"
     ]
    }
   ],
   "source": [
    "header_map = {\n",
    "    \"meter no\": \"meter_id\",\n",
    "    \"meter_no\": \"meter_id\",\n",
    "    \"reading_time\": \"timestamp\",\n",
    "    \"kwh\": \"energy\"\n",
    "}\n",
    "\n",
    "pipeline = DataPipeline(r\"D:\\AI Projects\\powerthon\\VALIDATIONS\\Powerthon_IP_DATA_Adani_Project_V2.xlsx\", header_map)\n",
    "status, logs = pipeline.run()\n",
    "\n",
    "print(\"\\n Final Pipeline Status:\", status)\n",
    "print(\" Logs:\", logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f035d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
